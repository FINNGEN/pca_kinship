from utils import np,mapcount,basic_iterator,return_header,write_fam_samplelist,make_sure_path_exists,tmp_bash,identify_separator,merge_files
import pandas as pd
import csv
from scipy.spatial.distance import cdist
from scipy.stats import chi2
import os,shlex,subprocess
from collections import defaultdict



def all_outliers(args,rej_lists):
    '''
    Functions that merges all outliers into a final list.
    '''
    all_outliers = os.path.join(args.pca_outlier_path, args.name + '_total_ethnic_outliers.txt')
    with open(all_outliers,'wt') as o:
        for f in rej_lists:
            with open(f,'rt') as i:
                for line in i:
                    o.write(line)

    print(f'Total non-ethnic finns : {mapcount(all_outliers)}')

    return all_outliers

def detect_ethnic_outliers(args):
    '''
    Returns file that has list of FG samples that did NOT pass bayesian outlier detection.
    '''
    tg_pca_path=  os.path.join(args.pca_outlier_path, '1k_pca/')
    make_sure_path_exists(tg_pca_path)
    tg_pca_file = pca_round(args,tg_pca_path) #plink output root

    #####################
    # OUTLIER DETECTION #
    #####################
    outlier_samples,ethnic_outliers =  aberrant_outliers(args,tg_pca_file)
    return outlier_samples,ethnic_outliers

def detect_eur_outliers(args,aberrant_output):
    """
    Returns file with list of EUR outliers based on classifier.
    """
    eur_outlier_path = os.path.join(args.pca_outlier_path, "eur_pca/")
    make_sure_path_exists(eur_outlier_path)

    eur_pca = finnish_eur_classifier(args,eur_outlier_path,aberrant_output)
    
    return eur_pca


def finnish_eur_classifier(args,eur_outlier_path,aberrant_output):
    """
    Function that returns file with excluded FG samples that survived first round based on EUR/FIN ground truth classification.

    """

    finngen_eur_outliers = os.path.join(args.pca_outlier_path,args.name +  '_eur_outliers.txt')
    if not os.path.isfile(finngen_eur_outliers) or args.force:

        # RETURN EUROPEAN OUTLIERS  
        eur,fin,finngen = eur_outlier_path + 'eur.txt',  eur_outlier_path + 'fin.txt', eur_outlier_path + "finngen.txt" # files with survivors from different populations

        # write fam of survivors
        columns = "1,2,6" # columns with ID,OUTLIER_BOOL,POP
        survivors_cmd = f"cat {aberrant_output} | cut -f {columns} | grep FALSE"
        for pop,f in [("EUR",eur),("FIN",fin)]:
            tmp_bash(f"cat {aberrant_output} | grep -w {pop}  | cut -f1 |  awk '{{print $0,$NF}}'> {f}",False)
        # remainin FG samples
        tmp_bash(f"cat {survivors_cmd} |  grep -wf {args.merged_plink_file}.fg.samples |  cut -f1 |  awk '{{print $0,$NF}}'> {finngen}",False)

        eur_pca = pca_round(args,eur_outlier_path,finngen)
        args.force =True
        #PROJECT
        for tag,sample_file in [('finngen',finngen),('eur',eur),('fin',fin)]:
            project(args,eur_outlier_path,tag,sample_file,eur_pca)

        eur_outliers = fin_eur_probs(args,eur_outlier_path,args.pc_filter,args.finn_prob_filter)
        args.logging.debug(f"Eur outliers:{len(eur_outliers)}")
            
        with open(finngen_eur_outliers,'wt') as o:
            for s in eur_outliers:o.write('\t'.join((s,s)) + '\n')


    print(f'Finngen EUR outliers : {mapcount(finngen_eur_outliers)}' )

    return finngen_eur_outliers


def project(args,eur_outlier_path,tag,samples_keep,pca_output_file):
    '''
    Project eur/fin/finngen samples onto the space generated by finngen data.
    '''

    columns = range(5,5+args.pc_filter)
    # define projection command
    if not os.path.isfile(eur_outlier_path+ tag + '.sscore' ) or args.force:
        args.force = True 
        args.logging.debug(tag)
        cmd = f'plink2 --bfile {args.merged_plink_file} --keep {samples_keep} --score {pca_output_file+".eigenvec.var"} 2 3 header-read no-mean-imputation  --score-col-nums {columns[0]}-{columns[-1]} --out {eur_outlier_path+tag}'
        args.logging.debug(cmd)
        subprocess.call(shlex.split(cmd),stdout=subprocess.DEVNULL)
        cmd = f'cat {eur_outlier_path+tag + ".sscore"}  | cut -f2,{",".join(list(map(str,columns)))} >  {eur_outlier_path+ tag}.eigenvec'
        tmp_bash(cmd,False)
        


def aberrant_outliers(args,tg_pca_file):
    '''
    Runs the outlier detection with aberrant R package
    '''

    # RUN ABERRANT PROPER
    outlier_samples = tg_pca_file +'_outlier_samples.tsv'
    if not os.path.isfile(outlier_samples) or args.force:
        args.force = True
        iterations = ' 1000 -p' if args.test else ' 3000 '
        args.logging.info(f'generating outliers at {tg_pca_file}')
        cmd  =f"""Rscript {os.path.join(args.parent_path,'scripts/pca_outlier_detection/scripts/classify_outliers.R')}  -f {tg_pca_file+".eigenvec"} -e {tg_pca_file+".eigenval"} -s {args.annot_pop} --n_iterations {iterations}  -o {tg_pca_file} --lambda {args.aberrant_lambda}"""
        args.logging.debug(cmd)
        subprocess.call(shlex.split(cmd))

   
    #WRITE FINNGEN OUTLIER SAMPLES
    ethnic_pca_outliers = os.path.join(args.pca_outlier_path, args.name + '_ethnic_outliers.txt')
    if not os.path.os.path.isfile(ethnic_pca_outliers) or args.force:
        args.force = True

        columns = "1,2,6" # columns with ID,OUTLIER_BOOL,POP
        survivors_cmd = f"""cat {outlier_samples} | cut -f {columns} | grep TRUE | grep -wf {args.merged_plink_file}.fg.samples | cut -f 1 | awk '{{print $0,$NF}}'  > {ethnic_pca_outliers}"""
        tmp_bash(survivors_cmd,False)
        
    else:
        pass
    
    print(f'Finngen ethnic outliers : {mapcount(ethnic_pca_outliers)}' )

    return outlier_samples,ethnic_pca_outliers


def pca_round(args,tg_pca_path,keep = None):
    '''
    Function that runs PCA on the merged dataset, with option to keep samples for iterative runs.
    '''

    tg_pca_file = os.path.join(tg_pca_path,args.name)

    if mapcount(tg_pca_file+ '.eigenvec') == 0 or args.force:
        args.force = True 
        #individuals that need to be removed
        k =  f' --keep {keep}' if keep is not None else ""            
        args.logging.debug(k)
            
        cmd = f'plink2 --bfile {args.merged_plink_file} --read-freq  {args.merged_plink_file}.afreq {k} --pca {args.pca_components} approx biallelic-var-wts --threads {args.cpus}  -out {tg_pca_file}'
        args.logging.debug(cmd)
        subprocess.call(shlex.split(cmd))
        
    return tg_pca_file

def build_superpop(args):
    '''
    Writes pop info for outlier detection.
    Returns filepath of file needed for outlier detection.
    '''
    
    # sample to super pop mapping file
    annot_pop =  os.path.join(args.misc_path, args.name + '_sample_annot.tsv')
    args.logging.info(f'generating super pop dict {annot_pop}')


    # sample info for tg
    #g_pop = os.path.join(args.data_path,'20130606_sample_info.txt')
    # get index of pop column and build sample to population dictionary
    cols = [return_header(args.tg_pop).index(elem) for elem in ['Sample','Population']]
    pop_dict = {sample:pop for sample,pop in basic_iterator(args.tg_pop,columns = cols)}
    
    # now i build a sample to pop dictionary where i keep superpop unless it's a Finn
    with open(annot_pop,'wt') as o:
        o.write('IID' +'\t' + 'SuperPops\n')

        # read in sample tags and assign "other" if missing
        tag_dict = defaultdict(lambda:"Other")
           
        # loop through input tg fam file and update population data
        for sample in basic_iterator(args.tg_bed.replace('.bed','.fam'),columns = 1):
            tag_dict[sample] = pop_dict[sample]

        # read in sample data
        for sample,tag in basic_iterator(args.sample_info,separator =identify_separator(args.sample_info)):
            tag_dict[sample] = tag

        # now i loop through all samples fro merged plink file and assign to each id a tag
        with open(args.merged_plink_file + '.id') as i:
            for line in i:
                sample = line.strip().split()[0]
                o.write(sample+ '\t' + tag_dict[sample] + '\n')

    # read in annotation for all samples
    with open(annot_pop) as i: tag_dict = {sample:pop for sample,pop in (line.strip().split() for line in i)}
    fg_samples = []
    with open(args.merged_plink_file + ".fg.samples") as fam:
        fg_samples.append(line.split(maxsplit=1)[0])
    fg_tags = {tag_dict[sample] for sample in fg_samples}
    return annot_pop,fg_tags


def fin_eur_probs(args,eur_outlier_path,pc_filter,finn_prob_filter):

    '''
    Returns probability of being a FIN based on mahalanobis distance to EUR and FIN centroids of finngen samples
    '''

    # read in EUR projection data
    eur_data = np.loadtxt(eur_outlier_path+ 'eur.eigenvec',dtype = float, skiprows = 1,usecols = range(1,pc_filter+1))
    eur_avg = np.reshape(np.average(eur_data,axis =0),(1,eur_data.shape[1]))
    eur_cov = np.linalg.inv(np.cov(eur_data.T))
    # read in FIN projection data
    fin_data = np.loadtxt(eur_outlier_path+ 'fin.eigenvec',dtype = float, skiprows = 1,usecols = range(1,pc_filter+1))      
    fin_avg = np.reshape(np.average(fin_data,axis = 0),(1,fin_data.shape[1]))
    fin_cov = np.linalg.inv(np.cov(fin_data.T))
    # read in FINNGEN sample data as ground truth
    finngen_data = np.loadtxt(eur_outlier_path+ 'finngen.eigenvec',dtype = float, skiprows = 1,usecols = range(1,pc_filter+1))
    # MAHALANOBIS DISTANCES
    eur_dist = cdist(finngen_data,eur_avg,metric = 'mahalanobis',VI = eur_cov).flatten()**2
    fin_dist = cdist(finngen_data,fin_avg,metric = 'mahalanobis',VI = fin_cov).flatten()**2
    # PROBS
    p_eur =  1 - chi2.cdf(eur_dist,pc_filter)
    p_fin =  1 - chi2.cdf(fin_dist,pc_filter)
    f_prob = p_fin/(p_fin + p_eur)
    # save data
    np.savetxt(args.misc_path + "eur.txt",p_eur)
    np.savetxt(args.misc_path + "fin.txt",p_fin)
    np.savetxt(args.misc_path + "prob.txt",f_prob)
    # return outliers
    fin_mask = (f_prob < finn_prob_filter)
    print(f'outliers: {fin_mask.sum()}')
    eur_outliers = np.loadtxt(eur_outlier_path+ 'finngen.eigenvec',dtype = str, skiprows = 1,usecols = 0 )[fin_mask]

    with open(eur_outlier_path + 'fin_bins.txt','wt') as o:
        fin_probs = f_prob[fin_mask]
        bins = np.linspace(0, 1, 11)
        digitized = np.digitize(fin_probs, bins)
        for j,bin_value in enumerate([len(fin_probs[digitized == i]) for i in range(1, len(bins))]):
            o.write(f"{round(bins[j],2)}\t{bin_value}\n")
        
    return eur_outliers


